{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración y limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CategoryID</th>\n",
       "      <th>CategoryName</th>\n",
       "      <th>ProductName</th>\n",
       "      <th>Price</th>\n",
       "      <th>Class</th>\n",
       "      <th>ModifyDate</th>\n",
       "      <th>Resistant</th>\n",
       "      <th>IsAllergic</th>\n",
       "      <th>VitalityDays</th>\n",
       "      <th>SalesID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>ProductID</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Date</th>\n",
       "      <th>TransactionNumber</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>MiddleInitial</th>\n",
       "      <th>LastName</th>\n",
       "      <th>CityID</th>\n",
       "      <th>Address</th>\n",
       "      <th>CityName</th>\n",
       "      <th>Zipcode</th>\n",
       "      <th>CountryID</th>\n",
       "      <th>CountryName</th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>State</th>\n",
       "      <th>pib</th>\n",
       "      <th>rpc</th>\n",
       "      <th>wti</th>\n",
       "      <th>Unemployment_Rate</th>\n",
       "      <th>Population_2018</th>\n",
       "      <th>personal_income</th>\n",
       "      <th>Crecimiento_poblacional</th>\n",
       "      <th>Total_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12630</td>\n",
       "      <td>24490</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>5BSK7H5X44DGRUWEKJEA</td>\n",
       "      <td>Jill</td>\n",
       "      <td>P</td>\n",
       "      <td>Soto</td>\n",
       "      <td>14</td>\n",
       "      <td>31 New Parkway</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>81678</td>\n",
       "      <td>32</td>\n",
       "      <td>United States</td>\n",
       "      <td>AR</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>1.225389e+14</td>\n",
       "      <td>1.534967e+10</td>\n",
       "      <td>4.628095</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6695497.0</td>\n",
       "      <td>3.403557e+06</td>\n",
       "      <td>0.56</td>\n",
       "      <td>227.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115884</td>\n",
       "      <td>95026</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>3Q0RRIMLEEIMZ4U2G347</td>\n",
       "      <td>Pamela</td>\n",
       "      <td>Z</td>\n",
       "      <td>Estrada</td>\n",
       "      <td>4</td>\n",
       "      <td>949 Milton Drive</td>\n",
       "      <td>Fremont</td>\n",
       "      <td>20641</td>\n",
       "      <td>32</td>\n",
       "      <td>United States</td>\n",
       "      <td>AR</td>\n",
       "      <td>California</td>\n",
       "      <td>9.364726e+14</td>\n",
       "      <td>1.991167e+10</td>\n",
       "      <td>4.628095</td>\n",
       "      <td>4.8</td>\n",
       "      <td>39461588.0</td>\n",
       "      <td>4.156992e+07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>811.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>217388</td>\n",
       "      <td>27676</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>T2ZK8X0HU1KWKPRQ5MUQ</td>\n",
       "      <td>Anita</td>\n",
       "      <td>B</td>\n",
       "      <td>Sanchez</td>\n",
       "      <td>10</td>\n",
       "      <td>30 West Milton Way</td>\n",
       "      <td>Toledo</td>\n",
       "      <td>52048</td>\n",
       "      <td>32</td>\n",
       "      <td>United States</td>\n",
       "      <td>AR</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>2.175128e+14</td>\n",
       "      <td>1.570400e+10</td>\n",
       "      <td>4.628095</td>\n",
       "      <td>5.2</td>\n",
       "      <td>11676341.0</td>\n",
       "      <td>6.392306e+06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>259.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>364759</td>\n",
       "      <td>11630</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>ILPQKU2EBTVNMTN7FQNL</td>\n",
       "      <td>Dustin</td>\n",
       "      <td>B</td>\n",
       "      <td>Coffey</td>\n",
       "      <td>40</td>\n",
       "      <td>904 Oak Parkway</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>51352</td>\n",
       "      <td>32</td>\n",
       "      <td>United States</td>\n",
       "      <td>AR</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>2.175128e+14</td>\n",
       "      <td>1.570400e+10</td>\n",
       "      <td>4.628095</td>\n",
       "      <td>5.2</td>\n",
       "      <td>11676341.0</td>\n",
       "      <td>6.392306e+06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>77.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>447481</td>\n",
       "      <td>83733</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>TI5RNV3CZM39NW16FG9M</td>\n",
       "      <td>Bridgette</td>\n",
       "      <td>X</td>\n",
       "      <td>Valenzuela</td>\n",
       "      <td>44</td>\n",
       "      <td>52 Rocky Second Drive</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>73999</td>\n",
       "      <td>32</td>\n",
       "      <td>United States</td>\n",
       "      <td>AR</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>1.203899e+14</td>\n",
       "      <td>1.524400e+10</td>\n",
       "      <td>4.628095</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6771631.0</td>\n",
       "      <td>3.328174e+06</td>\n",
       "      <td>0.94</td>\n",
       "      <td>570.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CategoryID CategoryName            ProductName   Price Class  \\\n",
       "0           1  Confections  Spoon - Soup, Plastic  32.442   Low   \n",
       "1           1  Confections  Spoon - Soup, Plastic  32.442   Low   \n",
       "2           1  Confections  Spoon - Soup, Plastic  32.442   Low   \n",
       "3           1  Confections  Spoon - Soup, Plastic  32.442   Low   \n",
       "4           1  Confections  Spoon - Soup, Plastic  32.442   Low   \n",
       "\n",
       "                ModifyDate Resistant IsAllergic  VitalityDays  SalesID  \\\n",
       "0  2017-03-03 09:47:09.310      Weak       True           0.0    12630   \n",
       "1  2017-03-03 09:47:09.310      Weak       True           0.0   115884   \n",
       "2  2017-03-03 09:47:09.310      Weak       True           0.0   217388   \n",
       "3  2017-03-03 09:47:09.310      Weak       True           0.0   364759   \n",
       "4  2017-03-03 09:47:09.310      Weak       True           0.0   447481   \n",
       "\n",
       "   CustomerID  Quantity  ProductID  Discount        Date  \\\n",
       "0       24490         7         15       0.0  2018-01-01   \n",
       "1       95026        25         15       0.0  2018-01-01   \n",
       "2       27676         8         15       0.0  2018-01-01   \n",
       "3       11630         3         15       0.2  2018-01-01   \n",
       "4       83733        22         15       0.2  2018-01-01   \n",
       "\n",
       "      TransactionNumber  FirstName MiddleInitial    LastName  CityID  \\\n",
       "0  5BSK7H5X44DGRUWEKJEA       Jill             P        Soto      14   \n",
       "1  3Q0RRIMLEEIMZ4U2G347     Pamela             Z     Estrada       4   \n",
       "2  T2ZK8X0HU1KWKPRQ5MUQ      Anita             B     Sanchez      10   \n",
       "3  ILPQKU2EBTVNMTN7FQNL     Dustin             B      Coffey      40   \n",
       "4  TI5RNV3CZM39NW16FG9M  Bridgette             X  Valenzuela      44   \n",
       "\n",
       "                 Address      CityName  Zipcode  CountryID    CountryName  \\\n",
       "0         31 New Parkway  Indianapolis    81678         32  United States   \n",
       "1       949 Milton Drive       Fremont    20641         32  United States   \n",
       "2     30 West Milton Way        Toledo    52048         32  United States   \n",
       "3        904 Oak Parkway     Cleveland    51352         32  United States   \n",
       "4  52 Rocky Second Drive       Memphis    73999         32  United States   \n",
       "\n",
       "  CountryCode       State           pib           rpc       wti  \\\n",
       "0          AR     Indiana  1.225389e+14  1.534967e+10  4.628095   \n",
       "1          AR  California  9.364726e+14  1.991167e+10  4.628095   \n",
       "2          AR        Ohio  2.175128e+14  1.570400e+10  4.628095   \n",
       "3          AR        Ohio  2.175128e+14  1.570400e+10  4.628095   \n",
       "4          AR   Tennessee  1.203899e+14  1.524400e+10  4.628095   \n",
       "\n",
       "   Unemployment_Rate  Population_2018  personal_income  \\\n",
       "0                3.8        6695497.0     3.403557e+06   \n",
       "1                4.8       39461588.0     4.156992e+07   \n",
       "2                5.2       11676341.0     6.392306e+06   \n",
       "3                5.2       11676341.0     6.392306e+06   \n",
       "4                3.8        6771631.0     3.328174e+06   \n",
       "\n",
       "   Crecimiento_poblacional  Total_price  \n",
       "0                     0.56       227.09  \n",
       "1                     0.26       811.05  \n",
       "2                     0.14       259.54  \n",
       "3                     0.14        77.86  \n",
       "4                     0.94       570.98  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_data = os.path.join(current_dir, \"../data/raw/Dataframe_Final_Data.csv\")\n",
    "data = pd.read_csv(url_data, sep = ',')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6690599, 35)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener las dimensiones.\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6690599 entries, 0 to 6690598\n",
      "Data columns (total 35 columns):\n",
      " #   Column                   Dtype  \n",
      "---  ------                   -----  \n",
      " 0   CategoryID               int64  \n",
      " 1   CategoryName             object \n",
      " 2   ProductName              object \n",
      " 3   Price                    float64\n",
      " 4   Class                    object \n",
      " 5   ModifyDate               object \n",
      " 6   Resistant                object \n",
      " 7   IsAllergic               object \n",
      " 8   VitalityDays             float64\n",
      " 9   SalesID                  int64  \n",
      " 10  CustomerID               int64  \n",
      " 11  Quantity                 int64  \n",
      " 12  ProductID                int64  \n",
      " 13  Discount                 float64\n",
      " 14  Date                     object \n",
      " 15  TransactionNumber        object \n",
      " 16  FirstName                object \n",
      " 17  MiddleInitial            object \n",
      " 18  LastName                 object \n",
      " 19  CityID                   int64  \n",
      " 20  Address                  object \n",
      " 21  CityName                 object \n",
      " 22  Zipcode                  int64  \n",
      " 23  CountryID                int64  \n",
      " 24  CountryName              object \n",
      " 25  CountryCode              object \n",
      " 26  State                    object \n",
      " 27  pib                      float64\n",
      " 28  rpc                      float64\n",
      " 29  wti                      float64\n",
      " 30  Unemployment_Rate        float64\n",
      " 31  Population_2018          float64\n",
      " 32  personal_income          float64\n",
      " 33  Crecimiento_poblacional  float64\n",
      " 34  Total_price              float64\n",
      "dtypes: float64(11), int64(8), object(16)\n",
      "memory usage: 1.7+ GB\n"
     ]
    }
   ],
   "source": [
    "# Obtener información sobre tipos de datos y valores no nulos.\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminar duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar si hay duplicados y eliminarlos si los hubiese.\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso no encontramos duplicados en el conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminar columnas sin relevancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CategoryID', 'ProductName', 'Price', 'Class', 'ModifyDate',\n",
      "       'Resistant', 'IsAllergic', 'VitalityDays', 'Quantity', 'Discount',\n",
      "       'Date', 'CityName', 'CountryName', 'State', 'pib', 'rpc', 'wti',\n",
      "       'Unemployment_Rate', 'Population_2018', 'personal_income',\n",
      "       'Crecimiento_poblacional', 'Total_price'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CategoryID</th>\n",
       "      <th>ProductName</th>\n",
       "      <th>Price</th>\n",
       "      <th>Class</th>\n",
       "      <th>ModifyDate</th>\n",
       "      <th>Resistant</th>\n",
       "      <th>IsAllergic</th>\n",
       "      <th>VitalityDays</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Date</th>\n",
       "      <th>CityName</th>\n",
       "      <th>CountryName</th>\n",
       "      <th>State</th>\n",
       "      <th>pib</th>\n",
       "      <th>rpc</th>\n",
       "      <th>wti</th>\n",
       "      <th>Unemployment_Rate</th>\n",
       "      <th>Population_2018</th>\n",
       "      <th>personal_income</th>\n",
       "      <th>Crecimiento_poblacional</th>\n",
       "      <th>Total_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>United States</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>1.225389e+14</td>\n",
       "      <td>1.534967e+10</td>\n",
       "      <td>4.628095</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6695497.0</td>\n",
       "      <td>3.403557e+06</td>\n",
       "      <td>0.56</td>\n",
       "      <td>227.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Fremont</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>9.364726e+14</td>\n",
       "      <td>1.991167e+10</td>\n",
       "      <td>4.628095</td>\n",
       "      <td>4.8</td>\n",
       "      <td>39461588.0</td>\n",
       "      <td>4.156992e+07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>811.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Toledo</td>\n",
       "      <td>United States</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>2.175128e+14</td>\n",
       "      <td>1.570400e+10</td>\n",
       "      <td>4.628095</td>\n",
       "      <td>5.2</td>\n",
       "      <td>11676341.0</td>\n",
       "      <td>6.392306e+06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>259.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>United States</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>2.175128e+14</td>\n",
       "      <td>1.570400e+10</td>\n",
       "      <td>4.628095</td>\n",
       "      <td>5.2</td>\n",
       "      <td>11676341.0</td>\n",
       "      <td>6.392306e+06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>77.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>United States</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>1.203899e+14</td>\n",
       "      <td>1.524400e+10</td>\n",
       "      <td>4.628095</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6771631.0</td>\n",
       "      <td>3.328174e+06</td>\n",
       "      <td>0.94</td>\n",
       "      <td>570.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CategoryID            ProductName   Price Class               ModifyDate  \\\n",
       "0           1  Spoon - Soup, Plastic  32.442   Low  2017-03-03 09:47:09.310   \n",
       "1           1  Spoon - Soup, Plastic  32.442   Low  2017-03-03 09:47:09.310   \n",
       "2           1  Spoon - Soup, Plastic  32.442   Low  2017-03-03 09:47:09.310   \n",
       "3           1  Spoon - Soup, Plastic  32.442   Low  2017-03-03 09:47:09.310   \n",
       "4           1  Spoon - Soup, Plastic  32.442   Low  2017-03-03 09:47:09.310   \n",
       "\n",
       "  Resistant IsAllergic  VitalityDays  Quantity  Discount        Date  \\\n",
       "0      Weak       True           0.0         7       0.0  2018-01-01   \n",
       "1      Weak       True           0.0        25       0.0  2018-01-01   \n",
       "2      Weak       True           0.0         8       0.0  2018-01-01   \n",
       "3      Weak       True           0.0         3       0.2  2018-01-01   \n",
       "4      Weak       True           0.0        22       0.2  2018-01-01   \n",
       "\n",
       "       CityName    CountryName       State           pib           rpc  \\\n",
       "0  Indianapolis  United States     Indiana  1.225389e+14  1.534967e+10   \n",
       "1       Fremont  United States  California  9.364726e+14  1.991167e+10   \n",
       "2        Toledo  United States        Ohio  2.175128e+14  1.570400e+10   \n",
       "3     Cleveland  United States        Ohio  2.175128e+14  1.570400e+10   \n",
       "4       Memphis  United States   Tennessee  1.203899e+14  1.524400e+10   \n",
       "\n",
       "        wti  Unemployment_Rate  Population_2018  personal_income  \\\n",
       "0  4.628095                3.8        6695497.0     3.403557e+06   \n",
       "1  4.628095                4.8       39461588.0     4.156992e+07   \n",
       "2  4.628095                5.2       11676341.0     6.392306e+06   \n",
       "3  4.628095                5.2       11676341.0     6.392306e+06   \n",
       "4  4.628095                3.8        6771631.0     3.328174e+06   \n",
       "\n",
       "   Crecimiento_poblacional  Total_price  \n",
       "0                     0.56       227.09  \n",
       "1                     0.26       811.05  \n",
       "2                     0.14       259.54  \n",
       "3                     0.14        77.86  \n",
       "4                     0.94       570.98  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(columns = ['TransactionNumber', 'CategoryName', 'SalesID', 'CountryCode', 'CustomerID', 'ProductID', 'CityID', 'CountryID', 'FirstName', 'MiddleInitial', 'LastName', 'Address', 'Zipcode'], inplace=True)\n",
    "\n",
    "print(data.columns)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columnas que DESCARTAR y por qué:\n",
    "\n",
    "`TransactionNumber`: \n",
    "Es simplemente un identificador único de cada transacción, no aporta información útil para predecir demanda.\n",
    "\n",
    "\n",
    "`SalesID`, `CustomerID`, `ProductID`, `CityID`, `CountryID`:\n",
    "Son IDs arbitrarios que no tienen significado en sí mismos y pueden confundir al modelo.\n",
    "\n",
    "`CategoryName`, `CountryCode`: Tenemos la columna CategoryID  y Country NaME la cual serian estas mismas columna ya categorizadas.\n",
    "\n",
    "\n",
    "`FirstName`, `MiddleInitial`, `LastName`:\n",
    "Son nombres personales; no tienen valor predictivo y pueden introducir sesgo o problemas de privacidad.\n",
    "\n",
    "\n",
    "`Address`, `Zipcode`:\n",
    "Pueden contener información útil si se convierten en variables regionales o socioeconómicas, pero en bruto son de muy alta cardinalidad y difíciles de usar directamente. Mejor descartarlas por ahora, a menos que tengamos una forma clara de agruparlas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columnas que pueden ser de utilidad pero que hay que modificar:\n",
    "\n",
    "`ModifyDate`, `Date`:\n",
    "\n",
    "En su forma cruda no son útiles, pero podemos extraer de ellas variables como:\n",
    "Día de la semana\n",
    "Mes\n",
    "¿Es fin de semana o no?\n",
    "¿Es feriado o no?\n",
    "Entonces: descartarlas como strings, pero sacar variables derivadas antes.\n",
    "\n",
    "`ProductName`, `CountryName`, `CityName`, `State`:\n",
    "\n",
    "Considerar agruparlas o usar solo las más frecuentes. (Factorizarlas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columnas ÚTILES para predecir la demanda:\n",
    "\n",
    "`CategoryID`, `Class` → tipo de producto.\n",
    "\n",
    "`Resistant`, `IsAllergic`, `VitalityDays` → características del producto.\n",
    "\n",
    "`Price`, `Discount`, `Total_price` → precio y descuentos.\n",
    "\n",
    "`pib`, `rpc` (renta per cápita), `wti` (petróleo), `Unemployment_Rate`, `Population_2018`, `personal_income`, `Crecimiento_poblacional` → variables macroeconómicas que podrían correlacionar con la demanda.\n",
    "\n",
    "Variables derivadas del tiempo (`Date`, `ModifyDate`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3: Análisis de variables univariante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis sobre variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis sobre variables numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals_variables = data_limpia.select_dtypes(include = ['number']).drop(columns=['Outcome']).corr() # Con esta linea seleccionamos las columnas numericas del dataframe.\n",
    "\n",
    "def plot_numericas(data_set, variables_numericas):\n",
    "    \n",
    "    # Crear la figura con 1 columnas y 2 filas por variable.\n",
    "    fig, axis = plt.subplots(len(variables_numericas) * 2, 1, figsize=(8, len(variables_numericas) * 7))\n",
    "\n",
    "    # Definir límites de los ejes x para cada par de gráficos (histograma y boxplot).\n",
    "    # RECORDAR CAMBIAR LOS NOMBRES Y PONER LOS DE LAS COLUMNAS DE DF CORRESPONDIENTE.\n",
    "    x_limits = {\n",
    "        'age': (0, 100),   # Rango para el histograma y el boxplot de columna1...\n",
    "        'duration': (0, 2000),    \n",
    "        'campaign': (0, 20), \n",
    "        'pdays': (0, 2000),\n",
    "           \n",
    "        # Añadir más columnas y rangos si es necesario...\n",
    "    }\n",
    "\n",
    "    # Iterar sobre cada columna del DataFrame\n",
    "    for i, col in enumerate(variables_numericas):\n",
    "        index = i * 2\n",
    "        # Histograma en la primera fila\n",
    "        sns.histplot(data = data_set, x = col, kde = True, ax = axis[index])\n",
    "        axis[index].set_title(f'Histogram of {col}')\n",
    "        \n",
    "        # Establecer límites del eje x para el histograma\n",
    "        if col in x_limits:\n",
    "            axis[index].set_xlim(x_limits[col])  # Asigna el rango de valores personalizado al histograma\n",
    "\n",
    "        # Boxplot en la segunda fila\n",
    "        sns.boxplot(data = data_set, x = col, ax = axis[index + 1])\n",
    "        axis[index + 1].set_title(f'Boxplot of {col}')\n",
    "        \n",
    "        # Establecer límites del eje x para el boxplot (mismo rango que el histograma)\n",
    "        if col in x_limits:\n",
    "            axis[index + 1].set_xlim(x_limits[col])  # Asigna el mismo rango de valores al boxplot\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_numericas(data_limpia, numericals_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 4: Análisis de variables multivariante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis numérico-numérico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numericals_variables = data_limpia.select_dtypes(include = ['number']).drop(columns=['Outcome']).corr() # Con esta linea seleccionamos las columnas numericas del dataframe.\n",
    "\n",
    "  \n",
    "def plot_numerico_numerico(data_set, variables_numericas):\n",
    "\n",
    "    target = 'Outcome' # Recordar cambiar el target.     \n",
    "    \n",
    "    # Crear una figura con 1 columna y 2 filas por cada variable\n",
    "    fig, axis = plt.subplots(len(variables_numericas) * 2, 1, figsize=(8, (len(variables_numericas) * 5)))\n",
    "\n",
    "    # Crear un diagrama de dispersión múltiple                \n",
    "    for i, col in enumerate(variables_numericas):\n",
    "\n",
    "        # Regplot en la primera fila (fila 2 * i)\n",
    "        sns.regplot(ax = axis[i * 2], data = data_set, x = col, y = target)\n",
    "        axis[i * 2].set_title(f'Regplot of {col} vs {target}')\n",
    "        \n",
    "        # Heatmap en la segunda fila.\n",
    "        sns.heatmap(data_set[[col, target]].corr(), annot = True, fmt = \".2f\", ax = axis[i * 2 + 1], cbar = True)\n",
    "        axis[i * 2 + 1].set_title(f'Correlation Heatmap of {col} vs {target}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_numerico_numerico(data_limpia, numericals_variables)\n",
    "\n",
    "data_limpia.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis de posibles relaciones entre variables numericas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis categórico-categórico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combinaciones de la clase con varias predictoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(figsize = (10, 5), ncols = 2)\n",
    "\n",
    "sns.barplot(ax = axis[0], data = data_limpia, x = \"Outcome\", y = 'Glucose', hue = 'BMI')\n",
    "\n",
    "sns.barplot(ax = axis[1], data = data_limpia, x = \"Outcome\", y = 'BMI', hue = 'BloodPressure').set(ylabel = None)\n",
    "for tick in axis[1].get_xticklabels():\n",
    "    tick.set_rotation(90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Análisis de correlaciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matriz_correlacion(data_set):\n",
    "\n",
    "    corr_matrix = data_set.select_dtypes(include = ['number']).corr() # Con esta linea seleccionamos las columnas numericas del dataframe.\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(corr_matrix, annot = True, fmt = \".2f\", linewidths = 0.5, cmap = \"coolwarm\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_matriz_correlacion(data_limpia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Una vez analizada la correlación, analicemos los dos casos vistos para corroborar la teoría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una vez analizada la correlación, analicemos los dos casos vistos para corroborar la teoría:\n",
    "\n",
    "fig, axis = plt.subplots(figsize = (10, 5), ncols = 2)\n",
    "\n",
    "sns.regplot(ax = axis[0], data = data_limpia, x = \"Outcome\", y = \"Glucose\", scatter_kws={'edgecolor': 'k', 'alpha': 0.6})\n",
    "sns.regplot(ax = axis[1], data = data_limpia, x = \"Age\", y = \"Pregnancies\", scatter_kws={'edgecolor': 'k', 'alpha': 0.6})\n",
    "axis[0].grid(linestyle='--', alpha=0.7)\n",
    "axis[1].grid(linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairpolot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficar el pairplot.\n",
    "\n",
    "sns.pairplot(data = data_limpia)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 5: Ingeniería de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data[\"Embarked\"].fillna(total_data[\"Embarked\"].mode()[0], inplace = True) # RECORDANDO QUE MODE PARA LAS VARIABLES CATEGORICAS\n",
    "\n",
    "total_data[\"Fare\"].fillna(total_data[\"Fare\"].mean(), inplace = True) # Y MEDIA PARA LAS VARIABLES NUMERICAS\n",
    "\n",
    "total_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferencia de nuevas características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escalado de valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de valores:\n",
    "\n",
    "# Separar 'x_con_outliers' y 'x_sin_outliers' e 'Y' en train y test. (resultante 6 excels). \n",
    "\n",
    "num_variables = data_limpia.select_dtypes(include = 'number').drop(columns = ['Outcome'], errors='ignore').columns # Si quiero eliminar alguna columna rellenar la parte de drop.\n",
    "\n",
    "# Dividimos el conjunto de datos en muestras de train y test\n",
    "X_con_outliers = data_limpia_con_outliers.drop(\"Outcome\", axis = 1)[num_variables]\n",
    "X_sin_outliers = data_limpia_sin_outliers.drop(\"Outcome\", axis = 1)[num_variables]\n",
    "y = data_limpia_con_outliers[\"Outcome\"]\n",
    "\n",
    "X_train_con_outliers, X_test_con_outliers, y_train, y_test = train_test_split(X_con_outliers, y, test_size = 0.2, random_state = 42)\n",
    "X_train_sin_outliers, X_test_sin_outliers = train_test_split(X_sin_outliers, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_con_outliers.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_con_outliers.xlsx\", index = False)\n",
    "X_train_sin_outliers.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_sin_outliers.xlsx\", index = False)\n",
    "X_test_con_outliers.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_con_outliers.xlsx\", index = False)\n",
    "X_test_sin_outliers.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_sin_outliers.xlsx\", index = False)\n",
    "y_train.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/y_train.xlsx\", index = False)\n",
    "y_test.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/y_test.xlsx\", index = False)\n",
    "\n",
    "X_train_con_outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "num_variables = data_limpia.select_dtypes(include = 'number').drop(columns = ['Outcome'], errors='ignore').columns # Si quiero eliminar alguna columna rellenar la parte de drop.\n",
    "\n",
    "### NORMALIZAMOS EL DATAFRAME CON OUTLIERS Y LO GUARDAMOS\n",
    "normalizador_con_outliers = StandardScaler()\n",
    "normalizador_con_outliers.fit(X_train_con_outliers)   \n",
    "\n",
    "with open(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Models(Norm_scal)/normalizador_con_outliers.pkl\", \"wb\") as file: # Guardar el Normalizador. \n",
    "  pickle.dump(normalizador_con_outliers, file)\n",
    "\n",
    "X_train_con_outliers_norm = normalizador_con_outliers.transform(X_train_con_outliers)\n",
    "X_train_con_outliers_norm = pd.DataFrame(X_train_con_outliers_norm, index = X_train_con_outliers.index, columns = num_variables)\n",
    "\n",
    "X_test_con_outliers_norm = normalizador_con_outliers.transform(X_test_con_outliers)\n",
    "X_test_con_outliers_norm = pd.DataFrame(X_test_con_outliers_norm, index = X_test_con_outliers.index, columns = num_variables)\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_con_outliers_norm.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_con_outliers_norm.xlsx\", index = False)\n",
    "X_test_con_outliers_norm.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_con_outliers_norm.xlsx\", index = False)\n",
    "### NORMALIZAMOS EL DATAFRAME SIN OUTLIERS Y LO GUARDAMOS\n",
    "normalizador_sin_outliers = StandardScaler()\n",
    "normalizador_sin_outliers.fit(X_train_sin_outliers)\n",
    "\n",
    "with open(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Models(Norm_scal)/normalizador_sin_outliers.pkl\", \"wb\") as file: # Guardar el Normalizador. \n",
    "  pickle.dump(normalizador_sin_outliers, file)\n",
    "\n",
    "X_train_sin_outliers_norm = normalizador_sin_outliers.transform(X_train_sin_outliers)\n",
    "X_train_sin_outliers_norm = pd.DataFrame(X_train_sin_outliers_norm, index = X_train_sin_outliers.index, columns = num_variables)\n",
    "\n",
    "X_test_sin_outliers_norm = normalizador_sin_outliers.transform(X_test_sin_outliers)\n",
    "X_test_sin_outliers_norm = pd.DataFrame(X_test_sin_outliers_norm, index = X_test_sin_outliers.index, columns = num_variables)\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_sin_outliers_norm.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_sin_outliers_norm.xlsx\", index = False)\n",
    "X_test_sin_outliers_norm.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_sin_outliers_norm.xlsx\", index = False)\n",
    "\n",
    "X_train_con_outliers_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Escalado Mínimo-Máximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado Mínimo-Máximo:\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "num_variables = data_limpia.select_dtypes(include = 'number').drop(columns = ['Outcome'], errors='ignore').columns # Si quiero eliminar alguna columna rellenar la parte de drop.\n",
    "\n",
    "### ESCALAMOS EL DATAFRAME CON OUTLIERS Y LO GUARDAMOS\n",
    "scaler_con_outliers = MinMaxScaler()\n",
    "scaler_con_outliers.fit(X_train_con_outliers)\n",
    "\n",
    "with open(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Models(Norm_scal)/scaler_con_outliers.pkl\", \"wb\") as file: # Guardar el Escaler. \n",
    "  pickle.dump(scaler_con_outliers, file)\n",
    "  \n",
    "X_train_con_outliers_scal = scaler_con_outliers.transform(X_train_con_outliers)\n",
    "X_train_con_outliers_scal = pd.DataFrame(X_train_con_outliers_scal, index = X_train_con_outliers.index, columns = num_variables)\n",
    "\n",
    "X_test_con_outliers_scal = scaler_con_outliers.transform(X_test_con_outliers)\n",
    "X_test_con_outliers_scal = pd.DataFrame(X_test_con_outliers_scal, index = X_test_con_outliers.index, columns = num_variables)\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_con_outliers_scal.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_con_outliers_scal.xlsx\", index = False)\n",
    "X_test_con_outliers_scal.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_con_outliers_scal.xlsx\", index = False)\n",
    "\n",
    "### ESCALAMOS EL DATAFRAME SIN OUTLIERS Y LO GUARDAMOS\n",
    "scaler_sin_outliers = MinMaxScaler()\n",
    "scaler_sin_outliers.fit(X_train_sin_outliers)\n",
    "\n",
    "with open(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Models(Norm_scal)/scaler_sin_outliers.pkl\", \"wb\") as file: # Guardar el Escaler. \n",
    "  pickle.dump(scaler_sin_outliers, file)\n",
    "\n",
    "X_train_sin_outliers_scal = scaler_sin_outliers.transform(X_train_sin_outliers)\n",
    "X_train_sin_outliers_scal = pd.DataFrame(X_train_sin_outliers_scal, index = X_train_sin_outliers.index, columns = num_variables)\n",
    "\n",
    "X_test_sin_outliers_scal = scaler_sin_outliers.transform(X_test_sin_outliers)\n",
    "X_test_sin_outliers_scal = pd.DataFrame(X_test_sin_outliers_scal, index = X_test_sin_outliers.index, columns = num_variables)\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_sin_outliers_scal.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_sin_outliers_scal.xlsx\", index = False)\n",
    "X_test_sin_outliers_scal.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_sin_outliers_scal.xlsx\", index = False)\n",
    "\n",
    "X_train_con_outliers_scal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif, SelectKBest\n",
    "\n",
    "# Con un valor de k = 4 decimos implícitamente que queremos eliminar 1 característica1 del conjunto de datos.\n",
    "\n",
    "selection_model = SelectKBest(f_classif, k = 7)\n",
    "selection_model.fit(X_train_con_outliers, y_train)\n",
    "\n",
    "ix = selection_model.get_support()\n",
    "X_train_sel = pd.DataFrame(selection_model.transform(X_train_con_outliers), columns = X_train_con_outliers.columns.values[ix])\n",
    "X_test_sel = pd.DataFrame(selection_model.transform(X_test_con_outliers), columns = X_test_con_outliers.columns.values[ix])\n",
    "\n",
    "# GUARDO X_train_sel.columns\n",
    "\n",
    "columns_list = X_train_sel.columns.tolist() # Convierte el objeto Index a una lista.tolist()\n",
    "\n",
    "with open(\"feature_selection_k_7.json\", \"w\") as f:\n",
    "  json.dump(columns_list, f)\n",
    "\n",
    "X_train_sel.head()\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_sel.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_sel_k7.xlsx\", index = False)\n",
    "X_test_sel.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_sel_k7.xlsx\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
