{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración y limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/js/4g9f6f7d2vj886wxt_jkqkyc0000gn/T/ipykernel_1333/1148754393.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(url_data, sep = ',')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CategoryID</th>\n",
       "      <th>CategoryName</th>\n",
       "      <th>ProductName</th>\n",
       "      <th>Price</th>\n",
       "      <th>Class</th>\n",
       "      <th>ModifyDate</th>\n",
       "      <th>Resistant</th>\n",
       "      <th>IsAllergic</th>\n",
       "      <th>VitalityDays</th>\n",
       "      <th>SalesID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>ProductID</th>\n",
       "      <th>Discount</th>\n",
       "      <th>TotalPrice</th>\n",
       "      <th>Date_x</th>\n",
       "      <th>TransactionNumber</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>MiddleInitial</th>\n",
       "      <th>LastName</th>\n",
       "      <th>CityID</th>\n",
       "      <th>Address</th>\n",
       "      <th>CityName</th>\n",
       "      <th>Zipcode</th>\n",
       "      <th>CountryID</th>\n",
       "      <th>CountryName</th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>State</th>\n",
       "      <th>pib</th>\n",
       "      <th>rpc</th>\n",
       "      <th>wti</th>\n",
       "      <th>Unemployment Rate</th>\n",
       "      <th>Population_2018</th>\n",
       "      <th>personal_income</th>\n",
       "      <th>Crecimiento (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19077</td>\n",
       "      <td>98726</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DNPDCTGZSONAGKEWLEQ7</td>\n",
       "      <td>Thomas</td>\n",
       "      <td>R</td>\n",
       "      <td>Rangel</td>\n",
       "      <td>6</td>\n",
       "      <td>72 West White First Way</td>\n",
       "      <td>Austin</td>\n",
       "      <td>781</td>\n",
       "      <td>32</td>\n",
       "      <td>United States</td>\n",
       "      <td>AR</td>\n",
       "      <td>Texas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28628666.0</td>\n",
       "      <td>5.762952e+06</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125395</td>\n",
       "      <td>77870</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RSG6YQO7W8ZG9MUWJTLC</td>\n",
       "      <td>Oscar</td>\n",
       "      <td>B</td>\n",
       "      <td>Young</td>\n",
       "      <td>36</td>\n",
       "      <td>38 First Way</td>\n",
       "      <td>Detroit</td>\n",
       "      <td>72819</td>\n",
       "      <td>32</td>\n",
       "      <td>United States</td>\n",
       "      <td>AR</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9984072.0</td>\n",
       "      <td>5.713350e+06</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>136141</td>\n",
       "      <td>91603</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>J5U5RBSY3GOYU7LBMGH9</td>\n",
       "      <td>Mary</td>\n",
       "      <td>G</td>\n",
       "      <td>Herrera</td>\n",
       "      <td>50</td>\n",
       "      <td>670 White Oak Way</td>\n",
       "      <td>Cincinnati</td>\n",
       "      <td>83634</td>\n",
       "      <td>32</td>\n",
       "      <td>United States</td>\n",
       "      <td>AR</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11676341.0</td>\n",
       "      <td>6.392306e+06</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>139321</td>\n",
       "      <td>6206</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0DKN419DSBL9Y2BWSDPL</td>\n",
       "      <td>Ebony</td>\n",
       "      <td>P</td>\n",
       "      <td>Hancock</td>\n",
       "      <td>59</td>\n",
       "      <td>45 White New Parkway</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>56647</td>\n",
       "      <td>32</td>\n",
       "      <td>United States</td>\n",
       "      <td>AR</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2911359.0</td>\n",
       "      <td>1.403880e+06</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Confections</td>\n",
       "      <td>Spoon - Soup, Plastic</td>\n",
       "      <td>32.442</td>\n",
       "      <td>Low</td>\n",
       "      <td>2017-03-03 09:47:09.310</td>\n",
       "      <td>Weak</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>188395</td>\n",
       "      <td>19057</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RPPCBUJ4ZXY5JJA4HKK3</td>\n",
       "      <td>Lynn</td>\n",
       "      <td>K</td>\n",
       "      <td>Bonilla</td>\n",
       "      <td>61</td>\n",
       "      <td>39 Nobel Boulevard</td>\n",
       "      <td>Miami</td>\n",
       "      <td>6794</td>\n",
       "      <td>32</td>\n",
       "      <td>United States</td>\n",
       "      <td>AR</td>\n",
       "      <td>Florida</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21244317.0</td>\n",
       "      <td>1.608971e+07</td>\n",
       "      <td>1.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CategoryID CategoryName            ProductName   Price Class  \\\n",
       "0           1  Confections  Spoon - Soup, Plastic  32.442   Low   \n",
       "1           1  Confections  Spoon - Soup, Plastic  32.442   Low   \n",
       "2           1  Confections  Spoon - Soup, Plastic  32.442   Low   \n",
       "3           1  Confections  Spoon - Soup, Plastic  32.442   Low   \n",
       "4           1  Confections  Spoon - Soup, Plastic  32.442   Low   \n",
       "\n",
       "                ModifyDate Resistant IsAllergic  VitalityDays  SalesID  \\\n",
       "0  2017-03-03 09:47:09.310      Weak       True           0.0    19077   \n",
       "1  2017-03-03 09:47:09.310      Weak       True           0.0   125395   \n",
       "2  2017-03-03 09:47:09.310      Weak       True           0.0   136141   \n",
       "3  2017-03-03 09:47:09.310      Weak       True           0.0   139321   \n",
       "4  2017-03-03 09:47:09.310      Weak       True           0.0   188395   \n",
       "\n",
       "   CustomerID  Quantity  ProductID  Discount  TotalPrice Date_x  \\\n",
       "0       98726        25         15       0.1         0.0    NaN   \n",
       "1       77870        20         15       0.0         0.0    NaN   \n",
       "2       91603        24         15       0.0         0.0    NaN   \n",
       "3        6206         2         15       0.0         0.0    NaN   \n",
       "4       19057         5         15       0.0         0.0    NaN   \n",
       "\n",
       "      TransactionNumber FirstName MiddleInitial LastName  CityID  \\\n",
       "0  DNPDCTGZSONAGKEWLEQ7    Thomas             R   Rangel       6   \n",
       "1  RSG6YQO7W8ZG9MUWJTLC     Oscar             B    Young      36   \n",
       "2  J5U5RBSY3GOYU7LBMGH9      Mary             G  Herrera      50   \n",
       "3  0DKN419DSBL9Y2BWSDPL     Ebony             P  Hancock      59   \n",
       "4  RPPCBUJ4ZXY5JJA4HKK3      Lynn             K  Bonilla      61   \n",
       "\n",
       "                   Address    CityName  Zipcode  CountryID    CountryName  \\\n",
       "0  72 West White First Way      Austin      781         32  United States   \n",
       "1             38 First Way     Detroit    72819         32  United States   \n",
       "2        670 White Oak Way  Cincinnati    83634         32  United States   \n",
       "3     45 White New Parkway      Kansas    56647         32  United States   \n",
       "4       39 Nobel Boulevard       Miami     6794         32  United States   \n",
       "\n",
       "  CountryCode     State  pib  rpc  wti  Unemployment Rate  Population_2018  \\\n",
       "0          AR     Texas  NaN  NaN  NaN                NaN       28628666.0   \n",
       "1          AR  Michigan  NaN  NaN  NaN                NaN        9984072.0   \n",
       "2          AR      Ohio  NaN  NaN  NaN                NaN       11676341.0   \n",
       "3          AR    Kansas  NaN  NaN  NaN                NaN        2911359.0   \n",
       "4          AR   Florida  NaN  NaN  NaN                NaN       21244317.0   \n",
       "\n",
       "   personal_income  Crecimiento (%)  \n",
       "0     5.762952e+06             1.18  \n",
       "1     5.713350e+06             0.11  \n",
       "2     6.392306e+06             0.14  \n",
       "3     1.403880e+06             0.09  \n",
       "4     1.608971e+07             1.34  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_data = os.path.join(current_dir, \"../data/raw/Dataframe_Final_Data.csv\")\n",
    "data = pd.read_csv(url_data, sep = ',')\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las dimensiones.\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener información sobre tipos de datos y valores no nulos.\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminar duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si hay duplicados y eliminarlos si los hubiese.\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminar columnas sin relevancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3: Análisis de variables univariante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis sobre variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis sobre variables numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals_variables = data_limpia.select_dtypes(include = ['number']).drop(columns=['Outcome']).corr() # Con esta linea seleccionamos las columnas numericas del dataframe.\n",
    "\n",
    "def plot_numericas(data_set, variables_numericas):\n",
    "    \n",
    "    # Crear la figura con 1 columnas y 2 filas por variable.\n",
    "    fig, axis = plt.subplots(len(variables_numericas) * 2, 1, figsize=(8, len(variables_numericas) * 7))\n",
    "\n",
    "    # Definir límites de los ejes x para cada par de gráficos (histograma y boxplot).\n",
    "    # RECORDAR CAMBIAR LOS NOMBRES Y PONER LOS DE LAS COLUMNAS DE DF CORRESPONDIENTE.\n",
    "    x_limits = {\n",
    "        'age': (0, 100),   # Rango para el histograma y el boxplot de columna1...\n",
    "        'duration': (0, 2000),    \n",
    "        'campaign': (0, 20), \n",
    "        'pdays': (0, 2000),\n",
    "           \n",
    "        # Añadir más columnas y rangos si es necesario...\n",
    "    }\n",
    "\n",
    "    # Iterar sobre cada columna del DataFrame\n",
    "    for i, col in enumerate(variables_numericas):\n",
    "        index = i * 2\n",
    "        # Histograma en la primera fila\n",
    "        sns.histplot(data = data_set, x = col, kde = True, ax = axis[index])\n",
    "        axis[index].set_title(f'Histogram of {col}')\n",
    "        \n",
    "        # Establecer límites del eje x para el histograma\n",
    "        if col in x_limits:\n",
    "            axis[index].set_xlim(x_limits[col])  # Asigna el rango de valores personalizado al histograma\n",
    "\n",
    "        # Boxplot en la segunda fila\n",
    "        sns.boxplot(data = data_set, x = col, ax = axis[index + 1])\n",
    "        axis[index + 1].set_title(f'Boxplot of {col}')\n",
    "        \n",
    "        # Establecer límites del eje x para el boxplot (mismo rango que el histograma)\n",
    "        if col in x_limits:\n",
    "            axis[index + 1].set_xlim(x_limits[col])  # Asigna el mismo rango de valores al boxplot\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_numericas(data_limpia, numericals_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 4: Análisis de variables multivariante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis numérico-numérico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numericals_variables = data_limpia.select_dtypes(include = ['number']).drop(columns=['Outcome']).corr() # Con esta linea seleccionamos las columnas numericas del dataframe.\n",
    "\n",
    "  \n",
    "def plot_numerico_numerico(data_set, variables_numericas):\n",
    "\n",
    "    target = 'Outcome' # Recordar cambiar el target.     \n",
    "    \n",
    "    # Crear una figura con 1 columna y 2 filas por cada variable\n",
    "    fig, axis = plt.subplots(len(variables_numericas) * 2, 1, figsize=(8, (len(variables_numericas) * 5)))\n",
    "\n",
    "    # Crear un diagrama de dispersión múltiple                \n",
    "    for i, col in enumerate(variables_numericas):\n",
    "\n",
    "        # Regplot en la primera fila (fila 2 * i)\n",
    "        sns.regplot(ax = axis[i * 2], data = data_set, x = col, y = target)\n",
    "        axis[i * 2].set_title(f'Regplot of {col} vs {target}')\n",
    "        \n",
    "        # Heatmap en la segunda fila.\n",
    "        sns.heatmap(data_set[[col, target]].corr(), annot = True, fmt = \".2f\", ax = axis[i * 2 + 1], cbar = True)\n",
    "        axis[i * 2 + 1].set_title(f'Correlation Heatmap of {col} vs {target}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_numerico_numerico(data_limpia, numericals_variables)\n",
    "\n",
    "data_limpia.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis de posibles relaciones entre variables numericas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis categórico-categórico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combinaciones de la clase con varias predictoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(figsize = (10, 5), ncols = 2)\n",
    "\n",
    "sns.barplot(ax = axis[0], data = data_limpia, x = \"Outcome\", y = 'Glucose', hue = 'BMI')\n",
    "\n",
    "sns.barplot(ax = axis[1], data = data_limpia, x = \"Outcome\", y = 'BMI', hue = 'BloodPressure').set(ylabel = None)\n",
    "for tick in axis[1].get_xticklabels():\n",
    "    tick.set_rotation(90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Análisis de correlaciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matriz_correlacion(data_set):\n",
    "\n",
    "    corr_matrix = data_set.select_dtypes(include = ['number']).corr() # Con esta linea seleccionamos las columnas numericas del dataframe.\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(corr_matrix, annot = True, fmt = \".2f\", linewidths = 0.5, cmap = \"coolwarm\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_matriz_correlacion(data_limpia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Una vez analizada la correlación, analicemos los dos casos vistos para corroborar la teoría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una vez analizada la correlación, analicemos los dos casos vistos para corroborar la teoría:\n",
    "\n",
    "fig, axis = plt.subplots(figsize = (10, 5), ncols = 2)\n",
    "\n",
    "sns.regplot(ax = axis[0], data = data_limpia, x = \"Outcome\", y = \"Glucose\", scatter_kws={'edgecolor': 'k', 'alpha': 0.6})\n",
    "sns.regplot(ax = axis[1], data = data_limpia, x = \"Age\", y = \"Pregnancies\", scatter_kws={'edgecolor': 'k', 'alpha': 0.6})\n",
    "axis[0].grid(linestyle='--', alpha=0.7)\n",
    "axis[1].grid(linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairpolot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficar el pairplot.\n",
    "\n",
    "sns.pairplot(data = data_limpia)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 5: Ingeniería de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data[\"Embarked\"].fillna(total_data[\"Embarked\"].mode()[0], inplace = True) # RECORDANDO QUE MODE PARA LAS VARIABLES CATEGORICAS\n",
    "\n",
    "total_data[\"Fare\"].fillna(total_data[\"Fare\"].mean(), inplace = True) # Y MEDIA PARA LAS VARIABLES NUMERICAS\n",
    "\n",
    "total_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferencia de nuevas características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escalado de valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de valores:\n",
    "\n",
    "# Separar 'x_con_outliers' y 'x_sin_outliers' e 'Y' en train y test. (resultante 6 excels). \n",
    "\n",
    "num_variables = data_limpia.select_dtypes(include = 'number').drop(columns = ['Outcome'], errors='ignore').columns # Si quiero eliminar alguna columna rellenar la parte de drop.\n",
    "\n",
    "# Dividimos el conjunto de datos en muestras de train y test\n",
    "X_con_outliers = data_limpia_con_outliers.drop(\"Outcome\", axis = 1)[num_variables]\n",
    "X_sin_outliers = data_limpia_sin_outliers.drop(\"Outcome\", axis = 1)[num_variables]\n",
    "y = data_limpia_con_outliers[\"Outcome\"]\n",
    "\n",
    "X_train_con_outliers, X_test_con_outliers, y_train, y_test = train_test_split(X_con_outliers, y, test_size = 0.2, random_state = 42)\n",
    "X_train_sin_outliers, X_test_sin_outliers = train_test_split(X_sin_outliers, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_con_outliers.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_con_outliers.xlsx\", index = False)\n",
    "X_train_sin_outliers.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_sin_outliers.xlsx\", index = False)\n",
    "X_test_con_outliers.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_con_outliers.xlsx\", index = False)\n",
    "X_test_sin_outliers.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_sin_outliers.xlsx\", index = False)\n",
    "y_train.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/y_train.xlsx\", index = False)\n",
    "y_test.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/y_test.xlsx\", index = False)\n",
    "\n",
    "X_train_con_outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "num_variables = data_limpia.select_dtypes(include = 'number').drop(columns = ['Outcome'], errors='ignore').columns # Si quiero eliminar alguna columna rellenar la parte de drop.\n",
    "\n",
    "### NORMALIZAMOS EL DATAFRAME CON OUTLIERS Y LO GUARDAMOS\n",
    "normalizador_con_outliers = StandardScaler()\n",
    "normalizador_con_outliers.fit(X_train_con_outliers)   \n",
    "\n",
    "with open(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Models(Norm_scal)/normalizador_con_outliers.pkl\", \"wb\") as file: # Guardar el Normalizador. \n",
    "  pickle.dump(normalizador_con_outliers, file)\n",
    "\n",
    "X_train_con_outliers_norm = normalizador_con_outliers.transform(X_train_con_outliers)\n",
    "X_train_con_outliers_norm = pd.DataFrame(X_train_con_outliers_norm, index = X_train_con_outliers.index, columns = num_variables)\n",
    "\n",
    "X_test_con_outliers_norm = normalizador_con_outliers.transform(X_test_con_outliers)\n",
    "X_test_con_outliers_norm = pd.DataFrame(X_test_con_outliers_norm, index = X_test_con_outliers.index, columns = num_variables)\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_con_outliers_norm.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_con_outliers_norm.xlsx\", index = False)\n",
    "X_test_con_outliers_norm.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_con_outliers_norm.xlsx\", index = False)\n",
    "### NORMALIZAMOS EL DATAFRAME SIN OUTLIERS Y LO GUARDAMOS\n",
    "normalizador_sin_outliers = StandardScaler()\n",
    "normalizador_sin_outliers.fit(X_train_sin_outliers)\n",
    "\n",
    "with open(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Models(Norm_scal)/normalizador_sin_outliers.pkl\", \"wb\") as file: # Guardar el Normalizador. \n",
    "  pickle.dump(normalizador_sin_outliers, file)\n",
    "\n",
    "X_train_sin_outliers_norm = normalizador_sin_outliers.transform(X_train_sin_outliers)\n",
    "X_train_sin_outliers_norm = pd.DataFrame(X_train_sin_outliers_norm, index = X_train_sin_outliers.index, columns = num_variables)\n",
    "\n",
    "X_test_sin_outliers_norm = normalizador_sin_outliers.transform(X_test_sin_outliers)\n",
    "X_test_sin_outliers_norm = pd.DataFrame(X_test_sin_outliers_norm, index = X_test_sin_outliers.index, columns = num_variables)\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_sin_outliers_norm.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_sin_outliers_norm.xlsx\", index = False)\n",
    "X_test_sin_outliers_norm.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_sin_outliers_norm.xlsx\", index = False)\n",
    "\n",
    "X_train_con_outliers_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Escalado Mínimo-Máximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado Mínimo-Máximo:\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "num_variables = data_limpia.select_dtypes(include = 'number').drop(columns = ['Outcome'], errors='ignore').columns # Si quiero eliminar alguna columna rellenar la parte de drop.\n",
    "\n",
    "### ESCALAMOS EL DATAFRAME CON OUTLIERS Y LO GUARDAMOS\n",
    "scaler_con_outliers = MinMaxScaler()\n",
    "scaler_con_outliers.fit(X_train_con_outliers)\n",
    "\n",
    "with open(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Models(Norm_scal)/scaler_con_outliers.pkl\", \"wb\") as file: # Guardar el Escaler. \n",
    "  pickle.dump(scaler_con_outliers, file)\n",
    "  \n",
    "X_train_con_outliers_scal = scaler_con_outliers.transform(X_train_con_outliers)\n",
    "X_train_con_outliers_scal = pd.DataFrame(X_train_con_outliers_scal, index = X_train_con_outliers.index, columns = num_variables)\n",
    "\n",
    "X_test_con_outliers_scal = scaler_con_outliers.transform(X_test_con_outliers)\n",
    "X_test_con_outliers_scal = pd.DataFrame(X_test_con_outliers_scal, index = X_test_con_outliers.index, columns = num_variables)\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_con_outliers_scal.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_con_outliers_scal.xlsx\", index = False)\n",
    "X_test_con_outliers_scal.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_con_outliers_scal.xlsx\", index = False)\n",
    "\n",
    "### ESCALAMOS EL DATAFRAME SIN OUTLIERS Y LO GUARDAMOS\n",
    "scaler_sin_outliers = MinMaxScaler()\n",
    "scaler_sin_outliers.fit(X_train_sin_outliers)\n",
    "\n",
    "with open(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Models(Norm_scal)/scaler_sin_outliers.pkl\", \"wb\") as file: # Guardar el Escaler. \n",
    "  pickle.dump(scaler_sin_outliers, file)\n",
    "\n",
    "X_train_sin_outliers_scal = scaler_sin_outliers.transform(X_train_sin_outliers)\n",
    "X_train_sin_outliers_scal = pd.DataFrame(X_train_sin_outliers_scal, index = X_train_sin_outliers.index, columns = num_variables)\n",
    "\n",
    "X_test_sin_outliers_scal = scaler_sin_outliers.transform(X_test_sin_outliers)\n",
    "X_test_sin_outliers_scal = pd.DataFrame(X_test_sin_outliers_scal, index = X_test_sin_outliers.index, columns = num_variables)\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_sin_outliers_scal.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_sin_outliers_scal.xlsx\", index = False)\n",
    "X_test_sin_outliers_scal.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_sin_outliers_scal.xlsx\", index = False)\n",
    "\n",
    "X_train_con_outliers_scal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif, SelectKBest\n",
    "\n",
    "# Con un valor de k = 4 decimos implícitamente que queremos eliminar 1 característica1 del conjunto de datos.\n",
    "\n",
    "selection_model = SelectKBest(f_classif, k = 7)\n",
    "selection_model.fit(X_train_con_outliers, y_train)\n",
    "\n",
    "ix = selection_model.get_support()\n",
    "X_train_sel = pd.DataFrame(selection_model.transform(X_train_con_outliers), columns = X_train_con_outliers.columns.values[ix])\n",
    "X_test_sel = pd.DataFrame(selection_model.transform(X_test_con_outliers), columns = X_test_con_outliers.columns.values[ix])\n",
    "\n",
    "# GUARDO X_train_sel.columns\n",
    "\n",
    "columns_list = X_train_sel.columns.tolist() # Convierte el objeto Index a una lista.tolist()\n",
    "\n",
    "with open(\"feature_selection_k_7.json\", \"w\") as f:\n",
    "  json.dump(columns_list, f)\n",
    "\n",
    "X_train_sel.head()\n",
    "\n",
    "# GUARDAR LOS DATASETS\n",
    "X_train_sel.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_train_sel_k7.xlsx\", index = False)\n",
    "X_test_sel.to_excel(\"/Users/julian/Desktop/vs code/STREAMLIT_Project_26_Julian_Lopez/ML-WEBAPP-USING-STREAMLIT_Project_26_Julian_Lopez/data/Excels/X_test_sel_k7.xlsx\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
